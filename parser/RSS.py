#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import argparse
import time
import json
from datetime import datetime
import feedparser
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import mysql.connector
from mysql.connector import Error
import traceback
from dotenv import load_dotenv

load_dotenv()

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –æ—à–∏–±–æ–∫ ===
TRACE_URL = "https://server.brain-project.online/trace.php"
NODE_NAME = os.getenv("NODE_NAME", "rss_fed_loader")
EMAIL = os.getenv("ALERT_EMAIL", "vladyurjevitch@yandex.ru")

def send_error_trace(exc: Exception, script_name: str = "RSS.py"):
    logs = (
        f"Node: {NODE_NAME}\n"
        f"Script: {script_name}\n"
        f"Exception: {repr(exc)}\n\n"
        f"Traceback:\n{traceback.format_exc()}"
    )
    payload = {
        "url": "cli_script",
        "node": NODE_NAME,
        "email": EMAIL,
        "logs": logs,
    }
    print(f"\nüì§ [POST] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á—ë—Ç –æ–± –æ—à–∏–±–∫–µ –Ω–∞ {TRACE_URL}")
    try:
        response = requests.post(TRACE_URL, data=payload, timeout=10)
        print(f"‚úÖ [POST] –£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ! –°—Ç–∞—Ç—É—Å: {response.status_code}")
    except Exception as e:
        print(f"‚ö†Ô∏è [POST] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ—Ç—á—ë—Ç: {e}")

# === –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ + .env fallback ===
parser = argparse.ArgumentParser(description="Federal Reserve RSS Parser ‚Üí MySQL")
parser.add_argument("table_name", help="–ò–º—è —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤ –ë–î")
parser.add_argument("host", nargs="?", default=os.getenv("DB_HOST"), help="–•–æ—Å—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
parser.add_argument("port", nargs="?", default=os.getenv("DB_PORT", "3306"), help="–ü–æ—Ä—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
parser.add_argument("user", nargs="?", default=os.getenv("DB_USER"), help="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ë–î")
parser.add_argument("password", nargs="?", default=os.getenv("DB_PASSWORD"), help="–ü–∞—Ä–æ–ª—å –ë–î")
parser.add_argument("database", nargs="?", default=os.getenv("DB_NAME"), help="–ò–º—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
args = parser.parse_args()

if not all([args.host, args.user, args.password, args.database]):
    print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î (—á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏–ª–∏ .env)")
    sys.exit(1)

DB_CONFIG = {
    'host': args.host,
    'port': int(args.port),
    'user': args.user,
    'password': args.password,
    'database': args.database,
}

FEEDS_URL = "https://www.federalreserve.gov/feeds/feeds.htm"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

IGNORE_KEYWORDS = [
    "Data Download", "Inspector General", "Supervision", "Reporting Forms",
    "Board Meetings", "Charge-Off", "Legal Developments", "Enforcement Actions"
]

class RSSCollector:
    def __init__(self, table_name: str):
        self.table_name = table_name
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': USER_AGENT})
        self.init_db()

    def get_db_connection(self):
        return mysql.connector.connect(**DB_CONFIG)

    def init_db(self):
        try:
            with self.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS `{self.table_name}` (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        feed_title VARCHAR(255),
                        feed_url VARCHAR(255),
                        entry_title VARCHAR(255),
                        entry_link VARCHAR(500),
                        entry_guid VARCHAR(190) UNIQUE,
                        entry_description LONGTEXT,
                        full_text LONGTEXT,
                        published VARCHAR(100),
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        INDEX idx_feed (feed_url),
                        INDEX idx_published (published)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                """)
                conn.commit()
                print(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ `{self.table_name}` –≥–æ—Ç–æ–≤–∞.")
        except Error as err:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ë–î –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ: {err}")

    def clean_html_content(self, soup):
        for tag in soup.select('script, style, nav, header, footer, aside, .header, .footer, .breadcrumb, .social-share'):
            tag.decompose()
        return soup

    def get_full_text_playwright(self, url: str) -> str:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Playwright –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å JS-—Å–∞–π—Ç–æ–≤."""
        from playwright.sync_api import sync_playwright
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    user_agent=USER_AGENT,
                    locale="en-US",
                    timezone_id="UTC"
                )
                page = context.new_page()
                page.goto(url, timeout=30000)
                page.wait_for_timeout(1000)
                # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä
                page.evaluate("""() => {
                    const trash = document.querySelectorAll('nav, header, footer, .header, .footer, .breadcrumb, .social-share');
                    trash.forEach(el => el.remove());
                }""")
                text = ""
                selectors = ["#article", "#content .col-md-8", "#content", ".data-article"]
                for sel in selectors:
                    els = page.query_selector_all(sel)
                    for el in els:
                        t = el.text_content().strip()
                        if len(t) > 50:
                            text += t + "\n"
                    if len(text) > 100:
                        break
                if not text:
                    body = page.query_selector("body")
                    if body:
                        text = body.text_content().strip()
                browser.close()
                return text if len(text) > 50 else ""
        except Exception as e:
            print(f"   [Playwright Error] {e}")
            return ""

    def get_full_text(self, url: str) -> str:
        """–°–Ω–∞—á–∞–ª–∞ requests, –µ—Å–ª–∏ –Ω–µ—É–¥–∞—á–∞ ‚Äî Playwright."""
        try:
            resp = self.session.get(url, timeout=10)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                self.clean_html_content(soup)
                content = None
                for sel in ['div#content', 'div#article', 'div.col-md-8', 'main']:
                    found = soup.select_one(sel)
                    if found:
                        content = found
                        break
                if content:
                    text = content.get_text(separator=' ', strip=True)
                    if "Skip to main content" not in text and len(text) > 200:
                        return text
        except Exception:
            pass
        print(f"   -> –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Playwright –¥–ª—è: {url[-30:]}")
        return self.get_full_text_playwright(url)

    def process_feed(self, feed_url: str, feed_name: str):
        try:
            feed = feedparser.parse(feed_url)
            new_count = 0
            with self.get_db_connection() as conn:
                cursor = conn.cursor()
                for entry in feed.entries[:10]:  # —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10
                    entry_guid = (entry.get('id') or entry.link)[:190]
                    if not entry_guid:
                        continue
                    cursor.execute(f"SELECT id FROM `{self.table_name}` WHERE entry_guid = %s", (entry_guid,))
                    if cursor.fetchone():
                        continue
                    print(f" [{feed_name}] –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {entry.title[:50]}...")
                    full_text = ""
                    if 'link' in entry:
                        full_text = self.get_full_text(entry.link)
                    description = entry.get('description', '')
                    if not full_text:
                        full_text = BeautifulSoup(description, 'html.parser').get_text(strip=True)
                    published = entry.get('published') or entry.get('updated') or datetime.now().isoformat()
                    cursor.execute(f"""
                        INSERT INTO `{self.table_name}` 
                        (feed_title, feed_url, entry_title, entry_link, entry_guid,
                         entry_description, full_text, published)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    """, (feed_name, feed_url, entry.title, entry.link, entry_guid,
                          description, full_text, published))
                    new_count += 1
                if new_count > 0:
                    conn.commit()
                    print(f" -> –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {new_count} –∑–∞–ø–∏—Å–µ–π.")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ñ–∏–¥–∞ {feed_name}: {e}")

    def run_cycle(self):
        print("–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∏–¥–æ–≤...")
        try:
            resp = self.session.get(FEEDS_URL, timeout=15)
            soup = BeautifulSoup(resp.text, 'html.parser')
            feeds = []
            seen = set()
            for a in soup.find_all('a', href=True):
                href = a['href']
                name = a.text.strip()
                if any(ignored in name for ignored in IGNORE_KEYWORDS):
                    continue
                if href.endswith('.xml'):
                    full_url = urljoin("https://www.federalreserve.gov", href)
                    if full_url not in seen:
                        seen.add(full_url)
                        feeds.append({'url': full_url, 'name': name})
            print(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(feeds)} –ø–æ–ª–µ–∑–Ω—ã—Ö –ª–µ–Ω—Ç (–ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã, —Ä–µ—á–∏).")
            for feed in feeds:
                self.process_feed(feed['url'], feed['name'])
        except Exception as e:
            print(f"–°–±–æ–π —Ü–∏–∫–ª–∞: {e}")

def main():
    collector = RSSCollector(args.table_name)
    collector.run_cycle()
    print("\nüèÅ –ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")

if __name__ == "__main__":
    try:
        main()
    except SystemExit:
        pass
    except KeyboardInterrupt:
        print("\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e!r}")
        send_error_trace(e)
        sys.exit(1)